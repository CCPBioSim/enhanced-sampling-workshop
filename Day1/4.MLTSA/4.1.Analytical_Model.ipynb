{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330a7d30-8412-4b80-8698-97cc7ed522a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4.1 Analytical model for MLTSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb25baf-5ff1-435b-9fd9-0025eb9e01c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de71203-8b1c-4790-916f-1fac8b999f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/') # We add the src folder to the path python will search for modules\n",
    "import OneD_pot_data as oned\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825b18f5-126e-450c-9cef-91a95b7435c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aims of this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833caa5-b323-42f2-99d8-cfa0067cf295",
   "metadata": {},
   "source": [
    "This is an example notebook on how to generate data with the 1D analytical model to analyze relevant features at the transition state from mixed data.\n",
    "\n",
    "You will learn: \n",
    "1. Generate 1D complex data for analyzing and evaluating different ML models. \n",
    "2. Setup a ML model with Scikit-Learn (MLP and GBDT)\n",
    "3. Pipeline the MLTSA to obtain the relevant features correlated with the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6625e0e-2c9e-4a6f-ad08-7cd4f8c55c9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ba77b-06e0-485b-948a-6d75ba37a0a5",
   "metadata": {},
   "source": [
    "The Machine Learning Transition State Analysis [(MLTSA)](https://doi.org/10.1021/acs.jctc.1c00924) is a method that allows us to analyze relevant features for the transition state (TS) of a physical system from downhill (unbiased) simulations.\n",
    "This is specially useful when simulating many ligand binding/unbinding events in parallel, where we start simulations at the approximated TS, and sample whether the ligand binds (IN) or unbinds (OUT).\n",
    "We can then use this data to teach a ML model to predict the final outcome from early on data (near-TS). Before using this model on real MD data, to better understand the method we will now use an analytical model as an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4082a3dd-67ac-449f-ab1f-8c2ed9717bf8",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/SWvsDW.png\" alt=\"SWvsDW\" width=\"800\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d734c2-a1b3-492d-a21a-97452bce5de6",
   "metadata": {},
   "source": [
    "Let's suppose we have a system with multiple states described by both _(a)_ Single Well (SW) and _(b)_ Double Well (DW) potentials. To simulate a real life scenario in our analytical model, like we do with distances on other systems, we can draw as many SW and DW potentials from the system as we want (or have), but the reaction we are looking at may be governed by only **one** of the multiple DW potentials we can draw, while the rest (including SWs) are just **noise** and are unrelated to the reaction. Analogous to when working with real life problems, we will generate data on this potentials at the same time, and try to search for this relevant potential among the noise. \n",
    "\n",
    "\n",
    "The simulations we will run on each of the potentials are using the Brownian Overdamped Langevin Dynamics equation: \n",
    "\n",
    "<center>\n",
    "<img src=\"images/langevin.png\" alt=\"SWvsDW\" width=\"300\"/>\n",
    "</center>\n",
    "\n",
    "Where ùõæ = 0.01 is constant along x and ùúÇ(ùë°) is a number randomly sampled from a normal\n",
    "(Gaussian) distribution centred at 0 and the spread is 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc4dbe-c75a-4fbc-bf25-59159025e5fc",
   "metadata": {},
   "source": [
    "As one may already think, just using the coordinates straight from the data would inmidiately give away the relevant DW out of all the SWs/DWs available, their value would correlate with the state they would fall on by the end (left/right minima), which we will call IN/OUT. \n",
    "\n",
    "\n",
    "In order to make it harder for the ML model to learn the answer, we will linearly mix the coordinates generated from the different potentials, to generate new features in this way:\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"images/analytical_model.png\" alt=\"SWvsDW\" width=\"800\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "This way we can generate as many features (y<sub>feature</sub>) as we want and all potentials (y<sub>sw</sub> , y<sub>dw</sub>) will be differently **mixed** (different $\\alpha$). If you generate 180 potentials, two randomly chosen potentials would be mixed with different coefficients ($\\alpha$)each time. Among these, some of them will be mixed with the **relevant DW** (y*<sub>dw</sub>), which would made them **correlated to the trajectory outcome** (IN/OUT). The bigger the $\\alpha$ the more correlated to the answer a given y<sub>feature</sub> will be. These potentials are the ones we want to be able to pinpoint using the ML. On the left you have a representation of the trajectory coordinates on the original relevant DW (y*<sub>dw</sub>), how the data would look like when mixing it with a SW (y*<sub>dw</sub> and y<sub>sw</sub>) and how it would look like when mixed with other non relevant DW (y*<sub>dw</sub> and y<sub>dw</sub>).\n",
    "\n",
    "Additionally, to make it even harder, we will also **only use data closer to the beginning of the trajectories** (Shaded in blue), where both outcomes are mixed the most, so it would be very hard to guess which feature is giving the answer away by simply looking at them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580783a-810a-49e2-bc9d-64192c71706e",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2217c205-f571-4521-ad8c-b18199411805",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hands-on code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96219128-d58f-4cd6-8af1-e6ddc265dd92",
   "metadata": {},
   "source": [
    "The code we will use here is a snippet version of the code available in MLTSA's [GitHub repo](https://github.com/pedrojuanbj/MLTSA), you can install it with `pip install MLTSA` but for this workshop the code we will need can be found in the `/src/` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34bf66c-55db-4f6c-95f7-4d2899b9b3c9",
   "metadata": {},
   "source": [
    "### Generating the trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb65a0-a2aa-4717-96f5-25b6ad3ba653",
   "metadata": {},
   "source": [
    "We will start by defining the number of SW and DW potentials we want to have on our model. In order to do that we can call the `potentials` class which will need the total number of potentials we want to generate, the number of DW potentials to have in it, and the potential we want to be relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2092bf7c-ebba-46d1-be99-9730191efee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sets the potentials, don't re-run\n",
    "total_n_pots = 25\n",
    "n_DW = 2\n",
    "relevant_DW_n = 1\n",
    "#After defining the desired parameters we define the potentials accordingly\n",
    "pots = oned.potentials(total_n_pots, n_DW, relevant_DW_n, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b252f-a0ad-4ac6-8f4d-650936c23e76",
   "metadata": {},
   "source": [
    "We can check the different shapes of the potentials we generated using the .shape atribute, we will get the X and Y values for the defined potentials which we can use to plot and verify their shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adcd984-449e-48d1-adbb-1d0ae04f6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "pots_shapes = pots.shape #Checking the shapes\n",
    "plt.title(\"Some defined potentials\")\n",
    "plt.plot(pots_shapes[0][0], pots_shapes[0][1], label=\"First potential\"); #Each entry has the X and Y values of the potential\n",
    "plt.plot(pots_shapes[pots.relevant_id][0], pots_shapes[pots.relevant_id][1], label=\"Relevant potential\");\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22217f67-1b9e-4c27-b8c5-e74db264fa8d",
   "metadata": {},
   "source": [
    "As you can see, our relevant potential is indeed a DW, now that we defined them we have to generate trajectories on them starting from the \"TS\" (at 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04bbe85-b359-42ae-90fd-0eb59fe3b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims = 10 #Number of desired trajectories\n",
    "n_steps = 500 #Number of steps to run the trajectories for\n",
    "\n",
    "#This will create our original unmixed data\n",
    "#it also gives you the outcomes for the ran trajectories depending on your relevant DW\n",
    "simple_data, answers = pots.generate_data(n_sims, n_steps) #This generates simple data\n",
    "print(\"Data has the shape of\", simple_data.shape, \"as in (n_sims, n_pot, n_steps)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f7554-a03f-4274-9ef1-c9b5a9e7255a",
   "metadata": {},
   "source": [
    "Now that we generated the data we can plot it to see how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d294129-d34c-46f0-9c2b-74d3ac5b7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].plot(simple_data[0,0,:], color=\"r\", label=\"Sim 1, potential 1\")\n",
    "axs[1].plot(simple_data[0,pots.relevant_id,:], color=\"r\", label=\"Sim 1, Relevant Potential\")\n",
    "axs[0].plot(simple_data[-1,0,:], color=\"b\", label=\"Sim 2, potential 1\")\n",
    "axs[1].plot(simple_data[-1,pots.relevant_id,:], color=\"b\", label=\"Sim 2, Relevant Potential\")\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel(\"Simulation Steps\")\n",
    "axs[1].set_ylabel(\"Reaction Coordinate\")\n",
    "axs[0].set_xlabel(\"Simulation Steps\")\n",
    "axs[0].set_ylabel(\"Reaction Coordinate\")\n",
    "axs[0].set_ylim(0,1)\n",
    "axs[1].set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130177f7-f6bc-489c-b2be-6a722be67ebd",
   "metadata": {},
   "source": [
    "As you can see, the generated data looks good, and indeed just looking at the relevant potential coordinates, we can easily find the relevant value that separates both classes (IN/OUT), whereas the first potential is noisy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc4268-f42f-4b43-82ec-bc3eccc26038",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Let's make it hard to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a9dc4-4574-4d9c-a16c-da293ec4b1c7",
   "metadata": {},
   "source": [
    "We will now create the linear combination data from the given trajectories. For this we will use the `dataset` class, which needs the previously defined potentials, desired number of features and the degree of mixing (how many to combine per feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0bd6b5-c1a3-468e-adf4-ee5b415e96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the first set of data.\n",
    "# It creates the mixing coefficients don't re-run or they override\n",
    "n_features = 100 #Number of features to create\n",
    "degree_of_mixing = 2 # Degree of mixing coordinates\n",
    "\n",
    "#We specified the number of features wanted and how much they will mix\n",
    "oneD_dataset = oned.dataset(pots, n_features, degree_of_mixing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b9a26e-2045-4a5f-a2ad-74501f8b4b16",
   "metadata": {},
   "source": [
    "Now that they are defined, we can also directly generate as many new trajectories as we want by simply calling `.generate_linear()` on the new `oneD_dataset` object with the number of steps to run for and the number of simulations to generate. It also gives you the outcomes as well so it is conviniently doing everything for you so you can put it in your testing pipeline or generate new data as you iterate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57befa-41e2-4139-aa49-da7b78656abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 100\n",
    "n_steps = 500\n",
    "data, ans = oneD_dataset.generate_linear(n_simulations, n_steps)\n",
    "print(\"The data shape is \", data.shape, \"as in (n_sims, n_features, n_steps)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a873a3-1f44-41d7-95a8-33521a7ee3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We proceed to plot the values\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].plot(data[0,0,:], color=\"r\", label=\"Sim 1, feature 1\")\n",
    "axs[1].plot(data[0,-1,:], color=\"r\", label=\"Sim 1, feature 2\")\n",
    "axs[0].plot(data[1,0,:], color=\"b\", label=\"Sim 2, feature 1\")\n",
    "axs[1].plot(data[1,-1,:], color=\"b\", label=\"Sim 2, feature 2\")\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel(\"Simulation Steps\")\n",
    "axs[1].set_ylabel(\"Reaction Coordinate\")\n",
    "axs[0].set_xlabel(\"Simulation Steps\")\n",
    "axs[0].set_ylabel(\"Reaction Coordinate\")\n",
    "axs[0].set_ylim(0,1)\n",
    "axs[1].set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbc3748-72df-45e8-8319-833e2b78289d",
   "metadata": {},
   "source": [
    "Now it is way harder to find which one is relevant. Let's get to train the ML models to predict the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d8235-f8cc-4fe6-817c-84dae82f1ecd",
   "metadata": {},
   "source": [
    "### Forecasting the outcome of the trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2278fc5d-fccb-4921-b4e9-9effcd7c2e91",
   "metadata": {},
   "source": [
    "Our first step is defining the models to train. For this we will use Scikit-learn and their awesome library which allows for easy training of ML models. We will use 2 popular models MLP, a Multi-Layer Perceptron which is basically a Neural Network with 3 layers (input, hidden, output), and Gradient Boosting Decision Tree (GBDT) which is a decision tree model which passes by the data sequentially to find the best splits to predict the outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3542e78-5aa2-4471-9c0e-5e23fc2bf6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier #Multi-Layer Perceptron model\n",
    "from sklearn.ensemble import GradientBoostingClassifier #Decision Tree model\n",
    "from sklearn.model_selection import train_test_split #This will split our data randomly for training/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5752142-46e8-4717-b5a2-d6f21b7adf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPClassifier(verbose=False, max_iter=500, random_state=42) #You can modify Hyperparameters here\n",
    "GBDT = GradientBoostingClassifier(verbose=False, n_estimators=50) #You can modify Hyperparameters here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa7061-161b-445f-a56a-174aaf355c95",
   "metadata": {},
   "source": [
    "Now that our models are defined with the desired parameters, we have to prepare the data for training. As we said, we will be training at early times to evaluate how good the models are at predicting while checking if they can detect the correlated features at all. These models take only 2 dimensions, therefore we cannot predict per trajectory but rather try to predict based on single frames, so the data will have to be concatenated. `n_frames = n_steps*n_sims`. Remember X is our input data (features) and Y is our target. In this case Y will be if the single frames belong to an \"IN\" or \"OUT\" class. You can try to print them if it's easier to understand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8fe22-1f49-4048-bde4-b57c7a9e5cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_frame = [20, 50] #From when to when to train on\n",
    "X, Y = oneD_dataset.PrepareData(data, ans, time_frame, mode=\"Normal\")\n",
    "print(\"The input data is in the shape of X:\", X.shape,\"Y:\", Y.shape)\n",
    "print(\"It needs the shape of (n_frames, n_features) and (n_frames)\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b686cdda-9adf-49a3-abff-5f06980dfcd8",
   "metadata": {},
   "source": [
    "To train the models we simply have to call `.fit()` so Scikit-learn will take care of the training for us. But before we will split on training data and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4807bf1-cee3-412f-8267-0fc848fc1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data randomly for train/test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y) \n",
    "#########################################################\n",
    "#Training MLP\n",
    "print(\"Training MLP\")\n",
    "MLP.fit(X_train, Y_train)\n",
    "print(\"Training ended\")\n",
    "Y_pred = MLP.predict(X_test)\n",
    "MLP_accuracy = np.mean(Y_pred == Y_test)\n",
    "print(\"MLP did:\", MLP_accuracy*100, \"% accuracy on test\")\n",
    "##########################################################\n",
    "#Training GBDT\n",
    "print(\"Training GBDT\")\n",
    "GBDT.fit(X_train, Y_train)\n",
    "print(\"Training ended\")\n",
    "Y_pred = GBDT.predict(X_test)\n",
    "GBDT_accuracy = np.mean(Y_pred == Y_test)\n",
    "print(\"GBDT did:\", GBDT_accuracy*100, \"% accuracy on test\")\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd9478-5ac7-4b5f-a0ef-6e7193780c31",
   "metadata": {},
   "source": [
    "They both do a good job at predicting this early, keep in mind that MLP takes less time to train than GBDT. Okay, so we have been able to predict the outcome fromt the given training simulations, but how does it perform on new simulations it has never seen at all? We can easily generate new ones and see its accuracy at predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95d0dd-d8a1-434c-857e-a9feac06582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Creating new validating sims ######################\n",
    "n_simulations = 50 #Let's do 50 new simulations we will use the same length as before\n",
    "data_val, ans_val = oneD_dataset.generate_linear(n_simulations, n_steps)\n",
    "print(\"The data for validation's shape is \", data.shape, \"as in (n_sims, n_features, n_steps)\")\n",
    "############ Preparing the new data for the prediction ######################\n",
    "X_val, Y_val = oneD_dataset.PrepareData(data_val, ans_val, time_frame, mode=\"Normal\") #Also we will \n",
    "print(\"The input datafor validation is in the shape of X:\", X_val.shape,\"Y:\", Y_val.shape)\n",
    "print(\"It needs the shape of (n_frames, n_features) and (n_frames)\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eafa6f-5ed0-482c-9e72-88e16f22ea7e",
   "metadata": {},
   "source": [
    "Now we predict again the accuracies for the new validating simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee08ac-0a5b-4987-ba7e-8324f1e020a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "Y_pred = MLP.predict(X_val)\n",
    "MLP_accuracy_v = np.mean(Y_pred == Y_val)\n",
    "print(\"MLP did:\", MLP_accuracy*100, \"% accuracy on validation\")\n",
    "##########################################################\n",
    "Y_pred = GBDT.predict(X_val)\n",
    "GBDT_accuracy_v = np.mean(Y_pred == Y_val)\n",
    "print(\"GBDT did:\", GBDT_accuracy*100, \"% accuracy on validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e1e70-2342-42b4-add3-d7020cbba6b2",
   "metadata": {},
   "source": [
    "Now a summary to find out how the models are performing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88537727-5774-43a2-ba64-ebed072c3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "table = [['Model', 'Test accuracy (%)', 'Validation accuracy (%)'],\n",
    "         [\"Multi-Layer Perceptron (MLP)\",  \"{:.2f}\".format(MLP_accuracy*100), \"{:.2f}\".format(MLP_accuracy_v*100)],\n",
    "         [\"Gradient Boosted Decision Tree (GBDT)\", \"{:.2f}\".format(GBDT_accuracy*100), \"{:.2f}\".format(GBDT_accuracy_v*100)]]\n",
    "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6223560d-ccb1-4207-88eb-0bf6e56bb95c",
   "metadata": {},
   "source": [
    " Let's do the feature analysis now and try to see if they can detect the features correlated to the relevant DW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdb7d1-10b5-4f61-acce-bfb1692685c5",
   "metadata": {},
   "source": [
    "### Finding relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344f2a4-9207-4aa5-853d-3ca696d635fa",
   "metadata": {},
   "source": [
    "Thanks to the training of the GBDT we can simply plot the Gini Feature Importance and take a look at what's important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f66bfb-8676-41b1-86aa-e40017631aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MLTSA import ADROP\n",
    "from MLTSA import MLTSA_ADROP_Plot\n",
    "from MLTSA import MLTSA_FeatImp_Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed8bfb-a6a5-45e1-9c56-64d86906fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 70 #Percentage threshold to show textbox with correlation value\n",
    "MLTSA_FeatImp_Plot([GBDT.feature_importances_], oneD_dataset, pots, correlation_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c970a1-6a68-46f5-b716-042230c89315",
   "metadata": {},
   "source": [
    "If everything went correctly, you should be able to see that the GBDT has been able to detect at least the most important feature, which has the highest correlation to the relevant DW potential value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12127983-1559-4606-b6c8-cf66ac89251d",
   "metadata": {},
   "source": [
    "When trying to look at the relevant features for our MLP, we can't do it like GBDT, we instead rely on the accuracy drop (ADROP) also known as feature permutation/feature substitution. In this case what we do is we change one feature at a time so the feature stays with the global mean value across trajectories for all samples through time. This way we cancel the variance and the information from the feature is cancelled. Thus, if we re-predict the original training data with this modification one at a time, we can get a drop in accuracy, correlated with the importance the feature had for the model to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa5efad-7d60-432d-ba51-b0baf7624bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def ADROP(data, ans, model):\n",
    "    # Calculating the global means from training data\n",
    "    means_per_sim = np.mean(data.T, axis=0)\n",
    "    gmeans = np.mean(means_per_sim, axis=1)\n",
    "    temp_sim_data = np.copy(data)\n",
    "    \n",
    "    #Repredicting with the modified data\n",
    "    adrop = []\n",
    "    for y, d in tqdm(enumerate(temp_sim_data), desc=\"Recalculating accuracy\"):\n",
    "        mean_sim = []\n",
    "        for n, mean in enumerate(gmeans):\n",
    "            tmp_dat = np.copy(d)\n",
    "            tmp_dat[n, :] = mean\n",
    "            yy = np.zeros(len(tmp_dat.T)).astype(str)\n",
    "            yy[:] = ans[y]\n",
    "            res = model.score(tmp_dat.T, yy)\n",
    "            mean_sim.append(res)\n",
    "        adrop.append(mean_sim)\n",
    "        \n",
    "    return adrop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559c078-9396-4a67-a056-a4c8337f6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_drop = ADROP(data[:,:, time_frame[0]:time_frame[1]], ans, MLP) #Remember to use the data in the range we used for training. \n",
    "adrop = np.mean(accuracy_drop, axis=0) #Mean across trajectories\n",
    "adrop = adrop/np.max(adrop) #Calculating relative drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc0ec2-05c1-4888-a5c4-9c66200a78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 80 #Percentage threshold to show textbox with correlation value\n",
    "MLTSA_ADROP_Plot([adrop], oneD_dataset, pots, correlation_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18720774-550a-4dc8-967b-666c100791df",
   "metadata": {},
   "source": [
    "Comparing the two previous graphs, you can see that the GBDT is more sensitive to really important features, whereas MLP with ADROP can pinpoint almost all relevant correlated features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe92c5e-0c06-4ae3-b677-bfb6bcba9bbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optional tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce3dc6-3dab-4870-98d8-31138a67b55d",
   "metadata": {},
   "source": [
    "\n",
    "We recommend if time allows, or at home, to try the following:\n",
    "- Try changing the data generation parameters such as number of DW to higher numbers or the degree of mixing to higher numbers. How do the ML models react to it? Are they still so accurate?\n",
    "<!-- ***Answer:*** The accuracy drops as the complexity increases.-->\n",
    "- Try also changing the time_frame at which we predict, what is the accuracy at later times? What about earlier? The whole trajectory?.\n",
    "<!-- ***Answer:*** At later times it should be easier to predict whereas at earlier times it should be harder .-->\n",
    "- While increasing complexity, compare the accuracy with n_sim=50 to n_sim=250.\n",
    "<!-- ***Answer:*** Increasing the number of samples increases accuracy as well.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e6d3c-491b-4534-a050-527ca885844d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
