{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d9b5d5-34d1-46a0-afdc-b9485643b272",
   "metadata": {},
   "source": [
    "# 4.2 Understanding the Benzamidine-Trypsin Unbinding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fd75e5-3ad9-42db-a968-eaecdfbc74ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b51e4-16ef-446b-9173-f1147375964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('src/') # We add the src folder to the path python will search for modules\n",
    "from MLTSA import ADROP\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75167fa6-ae2f-4796-8d11-fe51e68414ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Aims of this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80535092-afa6-4b38-942a-e74d78fefd5e",
   "metadata": {},
   "source": [
    "This is an example notebook on how to apply MLTSA on protein MD data . You will get to train the model and interpret the results to infer relevant features to the trypsin-benzamidine unbinding path.\n",
    "\n",
    "You will learn how to: \n",
    "1. Load in protein data and prepare it for processing. \n",
    "2. Setup a ML model with Scikit-Learn (MLP and GBDT) to work with protein data.\n",
    "3. Pipeline the MLTSA to obtain the relevant features correlated with the different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b410f3e8-5eae-4055-91aa-c58fa840a616",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707284e-3b02-400e-9390-28c444e4f114",
   "metadata": {},
   "source": [
    "### What is MLTSA? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff2b12-92e3-47b1-9bdf-ce42635a9ab8",
   "metadata": {},
   "source": [
    "This is a graphical abstract from our [paper](https://doi.org/10.1021/acs.jctc.1c00924) that sums up our approach when trying to understand ligand unbinding. A python module with all the code wrapped up to be used with your data and more in detail tutorials can be found at MLTSA's [GitHub repo](https://github.com/pedrojuanbj/MLTSA) :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b1421-a035-4634-9870-313351e87604",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/MLTSA_paper_fig.png\" alt=\"MLTSA paper figure\" width=\"1000\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b85af-c808-4a06-ac67-4d23b47d80b6",
   "metadata": {},
   "source": [
    "The Machine Learning Transition State Analysis (MLTSA) is a methodology where we make us of machine learning (ML) methods to analyze the main collective variables (CVs) that drive a system through the transition state (TS) like the example we have seen in 4.1. For this, we make use of many trajectories starting from an approximated TS analysis (preferably from an optimized string run). These trajectories will end up mostly in 2 outcomes (IN/OUT) where the ligand either exits the binding site, or comes back to the original bound state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8b550-c79e-42dc-920d-0b2c5ac64a77",
   "metadata": {},
   "source": [
    "In this case we will apply this methodology to try to unveil the relevant CVs that drive the trypsin-benzamidine complex through its unbinding TS. Here is an overview of the system at hand:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64cd36-2278-40a2-8af7-44bd336974e7",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/trypsin_overview.png\" alt=\"trypsin overview\" width=\"700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c98f262-c2f4-4677-bf97-0d5a3a3a49a3",
   "metadata": {},
   "source": [
    "We will use this system we have previously worked with (unbinding and string). For this case we will assume **we don't have any previous knowledge on this system**, and we want to be able to identify the key factors deciding the fate of the ligand. To do this in an automated way, we calculate different CVs (descriptors) and apply the analysis method. A good starting point would be to calculate the **closest ligand-protein residue distance for each residue**, these would be our CVs.\n",
    "\n",
    "Don't worry we won't make you run the 150 downhill trajectories from the approximated TS, we have done that for you already. What we will give you instead is a **dataset of distances** with the closest ligand-protein residue distance we have already calculated for you. You can load the data and take a look if you wish. In a real life situation, **any descriptors** such as interatomic distances, angles, dihedrals, etc. would also work as features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3d591-a1f6-420d-a3f0-e3acda66c341",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da73596c-4810-4f7d-af98-30f2a693e632",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hands-on code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff41bde-d7dc-4a6c-9867-787fe826982c",
   "metadata": {},
   "source": [
    "### Step 1: Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1abee6-2362-4c1f-a725-a5f2102038a7",
   "metadata": {},
   "source": [
    "The data is simply sitting in an `.npy` binary file to save space. You can simply load it with `np.load()`. The simulation outcomes can also be found on a `.txt` file, you can manually open them and check for yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80865b-4b33-4a13-90df-05a556bba69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = np.load(\"data/downhill_allres1.npy\") #First part of the dataset\n",
    "data2 = np.load(\"data/downhill_allres2.npy\") #Second part of the dataset\n",
    "data = np.concatenate((data1, data2)) #We merge them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef0175-00bd-4f3f-935f-d398f050669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = np.loadtxt(\"data/downhill_labels.txt\", dtype=str) #Load outcomes\n",
    "print(\"Data has shape:\", data.shape)\n",
    "print(\"Outcome list has:\", len(outcomes), \"entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896ec10-18a0-476a-b510-d6803225784c",
   "metadata": {},
   "source": [
    "This data has the shape (n_sims, n_frames, n_features). The features in this dataset are distances between protein residues and the Benzamidine. We will take a look at the data to show you how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1dd9b-f940-4014-a688-12efa1f7aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0, data.shape[1])/100\n",
    "plt.plot(time, data[1, :, -1], label=\"IN\", c=\"r\") #First simulation and last feature\n",
    "plt.plot(time, data[-2, :, -1], label=\"OUT\", c=\"b\") #Last simulation and last feature \n",
    "plt.xlabel(\"Time (ns)\")\n",
    "plt.ylabel(\"Distance (nm)\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081277cd-0b08-4b25-87da-1d65f3a71f80",
   "metadata": {},
   "source": [
    "As you can see, for example the last feature already gives away the outcome at later times. However, at early times there is overlap between the features. We can plot the values of this last feature for every simulation for the very first part of the data and we will see the overlap between outcomes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270681ea-6188-4509-95ba-315f9b913cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data[:60, :100, -1], c=\"r\", label=\"IN\") #First 60 are IN\n",
    "plt.plot(data[-60:, :100, -1], c=\"b\", label=\"OUT\") #Last 60 are out\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"Distance (nm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8454829-9012-450c-bc75-09ad36a1545a",
   "metadata": {},
   "source": [
    "We will only use the early data for training purposes, so we will define it here, you can change it to experiment later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41ca04-2476-44e7-aefb-b322c080fbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data[:,1:25, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706e43b-9c5d-41b4-b2f1-fd99742f1b79",
   "metadata": {},
   "source": [
    "### Step 2: Train the ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb549ada-b6ab-4fdc-b169-335ce668ca73",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Preparing the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf45ba0-6c88-4cff-8682-661cb54415e2",
   "metadata": {},
   "source": [
    "As with the preivous example in 4.1, the data corresponds to trajectories. We will use the MLP and GBDT models to predict the outcome of the simulations. But these models can only input 2D data, so we will have to concatenate the frames of the trajectories and assign a state for each frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3899aaf-750d-43f5-91c5-903f465642e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate(training_data)\n",
    "template = []\n",
    "for answer in outcomes:\n",
    "    tmp = np.zeros(training_data.shape[1]).astype(str)\n",
    "    tmp[:] = answer\n",
    "    template.append(tmp)\n",
    "Y = np.concatenate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c65527-0cb7-4bab-9af1-f26db7447d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input data:\", X.shape,\"Outcomes:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab249a1b-17ae-4a50-97ec-1a966610907e",
   "metadata": {},
   "source": [
    "Now that we have prepared our X and Y for training, we have to create the training/test split so we can test how accurate our ML model is at predicting on samples it has never seen. For this we will use a very convinient function from scikit-learn `train_test_split()` which already splits the data randomly and gives you back the parts so you can use it further down the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a19fd-60e3-4453-a513-05799662aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=0) #33% will be stored for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06f6ab-80da-4510-b9c8-eed79cf402c2",
   "metadata": {},
   "source": [
    "#### Defining the ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98333b-fcc9-4a57-8332-7459a2318569",
   "metadata": {},
   "source": [
    "Now we will define our classifiers to forecast the outcomes with  `MLPClassifier()` and `GradientBoostingClassifier()` from scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331f51b-2f32-4544-be8d-7b1a16b59969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "MLP = MLPClassifier(random_state=42, verbose=1, max_iter=500, n_iter_no_change=50) #Neural Network (Multi-Layer Perceptron)\n",
    "GBDT = GradientBoostingClassifier(random_state=42, n_estimators=100, verbose=1) #Gradient Boosting Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d650718-cb33-4bac-a5ab-54bf0d0927b1",
   "metadata": {},
   "source": [
    "#### Training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c495ca-93e3-4fc0-825f-431a41101900",
   "metadata": {},
   "source": [
    "Now let's use `X_train` and `Y_train` to fit our models and then use the `X_test` to predict the outcome `Y_pred`, and compare it to `Y_test` to asses the accuracy score. This models have the `verbosity=True`  by default, set it to `verbosity=False` in the previous cell if you don't want get output status from the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fde114-95fd-4197-89fe-36c87f46953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MLP\")\n",
    "MLP.fit(X_train, Y_train) #Training until convergence done by sk-learn\n",
    "print(\"Evaluating\")\n",
    "Y_pred = MLP.predict(X_test) #Predicting with the model\n",
    "accuracy = Y_pred == Y_test #Assesing accuracy\n",
    "print(\"Accuracy on test is \", np.mean(accuracy)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46bc15d-96bb-41cf-a96a-9118673f3e51",
   "metadata": {},
   "source": [
    "To check if our NN has converged indeed we can take a look at the loss function. If your computer allows, you can run for longer iterations or with more data and see if the accuracy improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa9166-4225-4ac0-8034-d835ba747da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss through training\")\n",
    "plt.plot(MLP.loss_curve_)\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.xlabel(\"Iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820b2ee-0338-4e37-acdd-ba0385bb7f0a",
   "metadata": {},
   "source": [
    "Let's train the GBDT now, in the same way as we did with the MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911fa7d-93ef-4e19-b1db-58f9bb9af9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training GBDT\")\n",
    "GBDT.fit(X_train, Y_train)\n",
    "print(\"Evaluating\")\n",
    "Y_pred = GBDT.predict(X_test)\n",
    "accuracy = Y_pred == Y_test\n",
    "print(\"Accuracy on test is \", np.mean(accuracy)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4220dd03-65e0-4ba4-a621-525bd0b0ee7c",
   "metadata": {},
   "source": [
    "We can also monitor how the loss of the GBDT has been improving thourgh the different estimators used. If your computer allows, you can run for longer iterations or with more data and see if the accuracy improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef836b00-032e-4db3-b3d6-37996e6c2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Loss through training\")\n",
    "plt.plot(GBDT.train_score_)\n",
    "plt.xlabel(\"Score at each iteration\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0e045e-f892-4fe1-a15a-895e8ae458bd",
   "metadata": {},
   "source": [
    "### Step 3: Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3947c-7d4b-4a9a-9f77-2f96855cab40",
   "metadata": {},
   "source": [
    "#### GBDT Gini Feature Importances "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a691bb3-20b0-415c-bb17-15bc4a1c43c2",
   "metadata": {},
   "source": [
    "Since the training with GBDT comes with the calculated feature importances, we can simply call `model.feature_importances_` to retrieve them. Out of those we can check the most relevant ones. Usually these come in a 1/n_features, so they all add up to 1. Let's plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde74e6-ec08-4a3b-8546-e8d6239b446e",
   "metadata": {},
   "outputs": [],
   "source": [
    " #We need this to know what each feature means, this is the residue that the distance to the ligand originates from. \n",
    "feat_names = np.load(\"data/allres_features.npy\", allow_pickle=True)\n",
    "FIs = GBDT.feature_importances_ #Obtaining the importances\n",
    "#Now to plotting:\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(range(0, len(FIs)), FIs)\n",
    "plt.xticks(range(0, len(FIs))[::5], feat_names[::5], rotation=75)\n",
    "plt.xlabel(\"Closest ligand-residue distance\")\n",
    "plt.ylabel(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b10a2-c299-4b3a-a926-f4e9ef31b749",
   "metadata": {},
   "source": [
    "We may want to check now the top features, so let's look at the top 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d295d2bc-51cd-475c-8520-ed6aafe07796",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_features = 5\n",
    "GBDT_tops = np.argsort(-GBDT.feature_importances_)[:n_top_features]\n",
    "print(\"Most relevant features are:\", GBDT_tops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7593401-fb9f-427b-bd49-836d03c1dec4",
   "metadata": {},
   "source": [
    "#### MLP Accuracy Drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd7ae7b-5222-4ed8-a3e4-6ac57f3f51bd",
   "metadata": {},
   "source": [
    "For the MLP, however, we will have to do a bit more work. We will have to call the `ADROP()` function from the MLTSA package to calculate the accuracy drops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c275868-afa0-4b34-93d5-8c0dc0f104d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will calculate the global mean, and iterate through the data re-predicting one at a time\n",
    "accuracy_drop = ADROP(training_data, outcomes, MLP)\n",
    "ADs = np.mean(accuracy_drop, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdb7fb-e38f-4514-bc9d-43f6db1000fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Accuracy Drop\")\n",
    "plt.plot(range(0, len(ADs)), ADs)\n",
    "plt.xticks(range(0, len(ADs))[::5], feat_names[::5], rotation=75)\n",
    "plt.xlabel(\"Closest ligand-residue distance\")\n",
    "plt.ylabel(\"Accuracy (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b20c29-3905-4406-98fa-08c25b6c7e9c",
   "metadata": {},
   "source": [
    "We may want to check now the top features, so let's look at the top 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f5ffb-ccac-4403-be14-4c2b5369130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_features = 5\n",
    "MLP_tops = np.argsort(ADs)[:n_top_features]\n",
    "print(\"Most relevant features are:\", MLP_tops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e2515b-913c-4843-acd6-86ed1cd5ddef",
   "metadata": {},
   "source": [
    "#### Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672bce9-e0cc-475c-bbae-55b9e303aced",
   "metadata": {},
   "source": [
    "How do the results for GBDT and MLP look like? Are they similar? Let's make an all in one plot to compare them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0adbc2-10b1-4ceb-8bfb-eb98a6de5a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "fig = plt.figure(figsize=(14,5))\n",
    "gs = fig.add_gridspec(2, hspace=0)\n",
    "axs = gs.subplots(sharex=True)\n",
    "\n",
    "###FI stuff \n",
    "axs[0].plot(FIs, color=\"black\")\n",
    "axs[0].plot(GBDT_tops, FIs[GBDT_tops],\"o\",  color=\"red\", markersize=4)\n",
    "\n",
    "###AD stuff\n",
    "axs[1].plot(ADs, color=\"black\")\n",
    "axs[1].plot(MLP_tops, ADs[MLP_tops], \"o\",  color=\"red\", markersize=4)\n",
    "\n",
    "####Fancy decorations\n",
    "axs[0].set_ylabel(\"FIs\", multialignment='center')\n",
    "axs[1].set_ylabel(\"ADs\",  multialignment='center')\n",
    "axs[1].yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "axs[0].yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "plt.xticks(range(0, len(feat_names))[::5], feat_names[::5], rotation=85)\n",
    "plt.xlabel(\"Protein residue involved in distance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbce52c-5e7e-4281-a665-e9c55d754b61",
   "metadata": {},
   "source": [
    "We clearly see one feature that is the clear winner in this case. Let's take a look more in detail and get the histograms for that feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1e780-5de4-475a-a2bc-cb9dd4b87ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feature = GBDT_tops[0] #Introduce the feature you wish to check\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Histogram for training data\")\n",
    "sns.kdeplot(np.concatenate(training_data[:65,:,top_feature]), c=\"r\", label=\"IN\")\n",
    "sns.kdeplot(np.concatenate(training_data[-65:,:,top_feature]), c=\"b\", label=\"OUT\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea86b3-dae9-47df-a498-d04ee03f1ba7",
   "metadata": {},
   "source": [
    "Now we see that there is some overlap, where the IN gets towards a lower value and the OUT shifts to higher values. Let's visualize the 2 top features for both MLP and GBDT to see if the data can explain more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66440c8c-1841-419c-9a95-9e1510e83c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "top2_features = GBDT_tops[:2]\n",
    "\n",
    "plt.plot(training_data[:65,:,top2_features[0]], training_data[:65,:,top2_features[1]], \"o\", c=\"r\", label=\"IN\");\n",
    "plt.plot(training_data[-65:,:,top2_features[0]], training_data[-65:,:,top2_features[1]], \"o\", c=\"b\", label=\"OUT\");\n",
    "plt.xlabel(feat_names[top2_features[0]])\n",
    "plt.ylabel(feat_names[top2_features[1]])\n",
    "\n",
    "####Bit of magic for the legend\n",
    "custom = [\n",
    "    Line2D([0], [0], marker='o', color='r', label='Scatter', markerfacecolor='r', markersize=5, linewidth=0),\n",
    "    Line2D([0], [0], marker='o', color='b', label='Scatter', markerfacecolor='b', markersize=5, linewidth=0)\n",
    "         ]\n",
    "plt.legend(custom, ['IN', 'OUT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eddd65-e67a-4300-849f-bcc11282ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "top2_features = MLP_tops[:2]\n",
    "\n",
    "plt.plot(training_data[:65,:,top2_features[0]], training_data[:65,:,top2_features[1]], \"o\", c=\"r\");\n",
    "plt.plot(training_data[-65:,:,top2_features[0]], training_data[-65:,:,top2_features[1]], \"o\", c=\"b\");\n",
    "plt.xlabel(feat_names[top2_features[0]])\n",
    "plt.ylabel(feat_names[top2_features[1]])\n",
    "\n",
    "####Bit of magic for the legend\n",
    "custom = [\n",
    "    Line2D([0], [0], marker='o', color='r', label='Scatter', markerfacecolor='r', markersize=5, linewidth=0),\n",
    "    Line2D([0], [0], marker='o', color='b', label='Scatter', markerfacecolor='b', markersize=5, linewidth=0)\n",
    "         ]\n",
    "plt.legend(custom, ['IN', 'OUT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eadc51-fef3-4a3f-b8ab-580bf6f2c87c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optional tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da90782-e6ce-425e-9621-9e19ef683b55",
   "metadata": {},
   "source": [
    "\n",
    "We recommend if time allows, or at home, to try the following:\n",
    "- Try increasing/decreasing the number of iterations/estimators for GBDT and MLP, how does the accuracy change?\n",
    "<!-- ***Answer:*** The accuracy may increase when increasing the iterations and decrease when it has less trains .-->\n",
    "- Try also changing the time_frame at which we predict, what is the accuracy at later times? What about earlier? The whole trajectory?.\n",
    "<!-- ***Answer:*** At later times it should be easier to predict whereas at earlier times it should be harder .-->\n",
    "- When changing the time_frame, do the relevant features change?.\n",
    "<!-- ***Answer:***.-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd4f16c-a9bb-4729-845b-6e51f7f8cf52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
