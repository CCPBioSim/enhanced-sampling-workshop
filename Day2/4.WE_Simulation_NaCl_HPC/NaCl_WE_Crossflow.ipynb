{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a196ca3",
   "metadata": {},
   "source": [
    "# The Weighted Ensemble Method\n",
    "\n",
    "## Sodium Chloride Association Kinetics, with Crossflow\n",
    "\n",
    "This Notebook illustrates moving a WE workflow from a workstation/laptop to an HPC system. It assumes you have already run through the basic NaCl example.\n",
    "\n",
    "Since WE worflows typicallty involve running large numbers of short MD simulations each of which is independent of the others, a HPC platform is very useful - as long as a better method can be found for running each simulation than just submitting it to the HPC scheduler.\n",
    "\n",
    "The combination of `Crossflow` and `dask.distributed` can provide this. We don't have time to go into the details here, but basically `dask.distributed` provides a mechanism to start a personal 'cluster' within an HPC system, to which short jobs - tasks - can be submitted directly, bypassing the usual scheduler. `Crossflow` works with `dask.distributed` to create the individual tasks that run MD simulations on the cluster, and manages the transfer of input and output files around the cluster system.\n",
    "\n",
    "Here we will use `dask.distributed` not to make a cluster on a HPC system, but to create a 'mini-cluster' of just one worker process, running in the background on your current laptop/desktop. In addition, we will use a fast 'fake\" MD application in place of a real compute-intensive MD code. \n",
    "\n",
    "Porting this workflow to a real HPC system will involve little more than swapping out the code that creates the mini-cluster for code to create the genuine HPC cluster, and swapping out the code for the fake MD application for that for a real one. Each of these will only require changing a few lines of code (see later).\n",
    "\n",
    "### Part 0: Install WElib (if not done already):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a7989",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+http://github.com/CharlieLaughton/WElib.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5dd3c",
   "metadata": {},
   "source": [
    "### Part 1: FakeMD\n",
    "Crossflow allows you to write functions that make MD codes like Gromacs or Amber accessible from Python in much the same way as you use OpenMM. For simplicity we will illustrate this using a fake MD code, so there is no requirement for you to have a \"proper\" MD code installed.\n",
    "\n",
    "`FakeMD` pretends to be a simple MD code that you would normally run from the command line:\n",
    "\n",
    "    fake_md -c starting_coordinates -p topology -n nsteps -r final_coordinates\n",
    "    \n",
    "where:\n",
    "* `starting_coordinates` is a Gromacs .gro file or an Amber .ncrst file, or a NAMD .pdb file or suchlike,\n",
    "* `topology` is a Gromacs topology file or an Amber prmtop file or a NAMD psf file or suchlike,\n",
    "* `nsteps` is the number of MD steps to run, and\n",
    "* `final_coordinates` is a Gromacs .gro file or an Amber .ncrst file, or a NAMD .pdb file or suchlike.\n",
    "\n",
    "The MD done by `fake_md` is entirely bogus - the coordinates are just shifted around a bit randomly - but it can be used to quickly test that a workflow is working properly before its swapped out for a genuine MD code.\n",
    "\n",
    "Begin by (outside this notebook, probably) putting a copy of the `fake_md` script in a directory that is in your **PATH**. Make sure the permissions are set to make it executable. Then check `fake_md` is in your path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! which fake_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b95341",
   "metadata": {},
   "source": [
    "Now we import elements of the crossflow library we will need to turn `fake_md` from something that is run from the command line into something accessible directly from python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossflow.tasks import SubprocessTask\n",
    "from crossflow.clients import Client\n",
    "from crossflow.filehandling import FileHandler\n",
    "import time\n",
    "import mdtraj as mdt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd2085",
   "metadata": {},
   "source": [
    "Now we create a task to run `fake_md`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ccf715",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_md = SubprocessTask('fake_md -c input.ncrst -p input.prmtop -n 500 -r output.ncrst')\n",
    "fake_md.set_inputs(['input.ncrst', 'input.prmtop'])\n",
    "fake_md.set_outputs(['output.ncrst'])\n",
    "fh = FileHandler()\n",
    "inpcrd = fh.load('nacl_unbound.ncrst')\n",
    "prmtop = fh.load('nacl.parm7')\n",
    "fake_md.set_constant('input.prmtop', prmtop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669eb30",
   "metadata": {},
   "source": [
    "Now we start a single-worker `cluster` on the local machine, and start a `client` as a portal to send jobs to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "cluster = LocalCluster(n_workers=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37921c75",
   "metadata": {},
   "source": [
    "Check to see that the crossflow task can be run via the client. Error messages point to some troubleshooting being required..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_crds = client.submit(fake_md, inpcrd)\n",
    "print(final_crds.status)\n",
    "time.sleep(5)\n",
    "print(final_crds.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2254d1f0",
   "metadata": {},
   "source": [
    "### Part 2: Building the WE workflow\n",
    "Now we import WElib and other utilities that will be useful. Many are the same as those used for the simple double well potential example, but we have a crossflow-compatible version of the `Stepper` and `ProgressCoordinator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85163ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as mdt\n",
    "import numpy as np\n",
    "import time\n",
    "from WElib import Walker, FunctionProgressCoordinator, Recycler, StaticBinner, SplitMerger, Recorder\n",
    "from WElib.crossflow import CrossflowFunctionStepper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3388cde",
   "metadata": {},
   "source": [
    "Create some walkers, each begins in the initial, dissociated, state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223bfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = inpcrd\n",
    "\n",
    "n_reps = 5\n",
    "walkers = [Walker(initial_state, 1.0/n_reps) for i in range(n_reps)]\n",
    "for w in walkers:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cc15e",
   "metadata": {},
   "source": [
    "The progress coordinate will be the distance between the sodium and chloride ion. We create a function that, given a state (in this scenario, the restart coordinates file), uses MDTraj to calculate this distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b830513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pc_func(state, topology):\n",
    "    t = mdt.load(state, top=topology)\n",
    "    na_atom = 0 # index of the sodium atom in the system\n",
    "    cl_atom = 1 # index of the chloride ion in the system\n",
    "    r = mdt.compute_distances(t, [[na_atom, cl_atom]])[0][0]\n",
    "    return r\n",
    "\n",
    "progress_coordinator = FunctionProgressCoordinator(pc_func, prmtop)\n",
    "walkers = progress_coordinator.run(walkers)\n",
    "for w in walkers:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0236c584",
   "metadata": {},
   "source": [
    "We use the same bin boundaries as in the WESTPA tutorials. Notice these are closer-spaced at shorter distances, as the solvation shells get \"stiffer\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "binner = StaticBinner([0, 0.26, 0.28, 0.3, 0.32, 0.34, 0.36, 0.38, 0.4, 0.45, 0.5, \n",
    "                 0.55, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5])\n",
    "walkers = binner.run(walkers)\n",
    "for w in walkers:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb22fc2",
   "metadata": {},
   "source": [
    "We will recycle walkers when the Na-Cl distance falls below 0.26 nm. As the progress coordinate is something that gets smaller as we move towards the target state, this is a \"retrograde\" coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea473276",
   "metadata": {},
   "outputs": [],
   "source": [
    "recycler = Recycler(0.26, retrograde=True)\n",
    "walkers = recycler.run(walkers)\n",
    "for w in walkers:\n",
    "    print(w)\n",
    "print('recycled flux = ',recycler.flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06abc661",
   "metadata": {},
   "source": [
    "The SplitMerger is just the same as that used for the DWP example. We create it and run it, even though we know that at this time it will have nothing to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33049e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitmerger = SplitMerger(n_reps)\n",
    "walkers = splitmerger.run(walkers)\n",
    "for w in walkers:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc835df",
   "metadata": {},
   "source": [
    "We create a `stepper` from the `CrossflowFunctionStepper` class. The arguments are the Dask/Crosasflow `client` that connectes us to the cluster, the Crossflow `fake_md` task, and the extra arguments that task function takes (just the prmtop file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c1e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "stepper = CrossflowFunctionStepper(client, fake_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc8988",
   "metadata": {},
   "source": [
    "Then we will apply the stepper. Because we are using \"fake_md\", it should run fairly fast, whatever the spec of your laptop/desktop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba40071",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "new_walkers = stepper.run(walkers) # this is where the MD happens\n",
    "end_time = time.time()\n",
    "print(f'{len(walkers)} simulations completed in {end_time-start_time:6.1f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ed56d",
   "metadata": {},
   "source": [
    "Let's see where those MD steps have moved each walker to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da1c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_walkers = progress_coordinator.run(new_walkers)\n",
    "new_walkers = binner.run(new_walkers)\n",
    "new_walkers = recycler.run(new_walkers)\n",
    "print('recycled flux = ', recycler.flux)\n",
    "for w in new_walkers:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3683fe",
   "metadata": {},
   "source": [
    "Apply the SplitMerger to the list of walkers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ec460",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_walkers = splitmerger.run(new_walkers)\n",
    "for w in new_walkers:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339299c0",
   "metadata": {},
   "source": [
    "### Part 3: Iterating the WE workflow\n",
    "OK, that's all the components in place, they have been tested individually and seem to be behaving. Time to run a few cycles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f0abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cycles=10\n",
    "print(' cycle    n_walkers   left-most bin  right-most bin   flux')\n",
    "for i in range(n_cycles):\n",
    "    new_walkers = stepper.run(new_walkers)\n",
    "    new_walkers = progress_coordinator.run(new_walkers)\n",
    "    new_walkers = binner.run(new_walkers)\n",
    "    new_walkers = recycler.run(new_walkers)\n",
    "    if recycler.flux > 0.0:\n",
    "        new_walkers = progress_coordinator.run(new_walkers)\n",
    "        new_walkers = binner.run(new_walkers)\n",
    "    new_walkers = splitmerger.run(new_walkers)\n",
    "    occupied_bins = list(binner.bin_weights.keys())\n",
    "    print(f' {i:3d} {len(new_walkers):10d} {min(occupied_bins):12d} {max(occupied_bins):14d} {recycler.flux:20.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd7aa9",
   "metadata": {},
   "source": [
    "(You can ignore any warnings about time taken for garbage collection - they are due to the low CPU overhead of the fake MD code and will go away when a real MD code is used.)\n",
    "\n",
    "#### Porting to a real HPC system\n",
    "\n",
    "As mentioned above, if we were running this notebook on a HPC system, e.g. ARCHER2, we could now swap out the 'mini-cluster' for a real one, and the fake MD code for a real one. Here we outline the process.\n",
    "\n",
    "##### From fake MD to Gromacs\n",
    "First we would neeed to convert the Amber files `nacl_unbound.ncrst` and `nacl.parm7` to Gromacs equivalents - Amber provides some utilities to do this. \n",
    "\n",
    "Then we create a `Crossflow` task to run both `grompp` and `mdrun` steps of a Gromacs simulation. Details might vary, but it will be something like this:\n",
    "\n",
    "    rungmx = SubprocessTask('gmx grompp -f x.mdp -c x.gro -p x.top -o x.tpr -maxwarn 1; srun --distribution=block:block --hint=nomultithread gmx_mpi mdrun -s x.tpr -c y.gro')\n",
    "    rungmx.set_inputs(['x.gro', 'x.top'])\n",
    "    rungmx.set_outputs(['y.gro'])\n",
    "    fh = FileHandler()\n",
    "    inpcrd = fh.load('nacl_unbound.gro')\n",
    "    top = fh.load('nacl.top')\n",
    "    rungmx.set_constant('x.top', top)\n",
    "    \n",
    "##### From mini-cluster to HPC cluster\n",
    "Instructions on how to create a `dask.distributed` cluster on ARCHER2 are included in the user guide - see the section about `dask-jobqueue` [here](https://docs.archer2.ac.uk/user-guide/python/). Once you have got your virtual environment set up, etc., then to create a suitable `cluster` you would amend your Jupyter notebook something like this:\n",
    "\n",
    "        cluster = SLURMCluster(cores=1,\n",
    "                       job_cpu=1,\n",
    "                       processes=1,\n",
    "                       memory='256GB',\n",
    "                       queue='standard',\n",
    "                       header_skip=['-n ', '--mem'],\n",
    "                       interface='hsn0',\n",
    "                       job_extra_directives=['--nodes=1',\n",
    "                           '--qos=\"standard\"',\n",
    "                           '--tasks-per-node=128'],\n",
    "                       python='python',\n",
    "                       project='xxxxxx',  # put your account code in here\n",
    "                       walltime=\"01:00:00\",\n",
    "                       shebang=\"#!/bin/bash --login\",\n",
    "                       local_directory='$PWD',\n",
    "                       job_script_prologue=['module load gromacs',\n",
    "                          'export PYTHONUSERBASE=/some/work/directory/path/.local',\n",
    "                          'export PATH=$PYTHONUSERBASE/bin:$PATH',\n",
    "                          'export PYTHONPATH=$PYTHONUSERBASE/lib/<python_version>/site-packages:$PYTHONPATH',\n",
    "                          'export OMP_NUM_THREADS=1',\n",
    "                          'source /path/to/virtual/environment/bin/activate'])\n",
    "    \n",
    "        print('scaling cluster...')\n",
    "        cluster.scale(n_workers) # the number of worker processes - each will be one ARCHER2 node\n",
    "        client = Client(cluster)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77546ab",
   "metadata": {},
   "source": [
    "##### Create the new stepper\n",
    "\n",
    "Now you have a new `Crossflow` task to run a Gromacs simulation on Archer2, and the specification for an Archer2 cluster, the two can be used to create the WE `Stepper`:\n",
    "\n",
    "    stepper = CrossflowFunctionStepper(client, rungmx)\n",
    "    \n",
    "    \n",
    "And you should be good to go.\n",
    "\n",
    "Obviously in reality you are likely to run WE jobs on Archer2 from Python scripts rather than interactively through Jupyter notebooks, but the essential code will be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e7eae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
